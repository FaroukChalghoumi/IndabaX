{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "#@title Download data from GCP bucket\n",
        "import sys\n",
        "\n",
        "if 'google.colab' in sys.modules:\n",
        "  !gsutil -m cp -r gs://indaba-data .\n",
        "else:\n",
        "  !mkdir -p indaba-data/train\n",
        "  !wget -P indaba-data/train https://storage.googleapis.com/indaba-data/train/train.csv --continue\n",
        "  !wget -P indaba-data/train https://storage.googleapis.com/indaba-data/train/train_mut.pt --continue\n",
        "  !wget -P indaba-data/train https://storage.googleapis.com/indaba-data/train/train_wt.pt --continue\n",
        "\n",
        "  !mkdir -p indaba-data/test\n",
        "  !wget -P indaba-data/test https://storage.googleapis.com/indaba-data/test/test.csv --continue\n",
        "  !wget -P indaba-data/test https://storage.googleapis.com/indaba-data/test/test_mut.pt --continue\n",
        "  !wget -P indaba-data/test https://storage.googleapis.com/indaba-data/test/test_wt.pt --continue"
      ],
      "metadata": {
        "id": "DEk5q0rhjBWZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e5c3c82c-1d06-4330-8b0f-9c3c4383a82a"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Copying gs://indaba-data/README.txt...\n",
            "/ [0 files][    0.0 B/   33.0 B]                                                \rCopying gs://indaba-data/test/test.csv...\n",
            "/ [0 files][    0.0 B/290.0 KiB]                                                \rCopying gs://indaba-data/test/test_mut.pt...\n",
            "/ [0 files][    0.0 B/  9.6 MiB]                                                \rCopying gs://indaba-data/test/test_wt.pt...\n",
            "Copying gs://indaba-data/train/train_mut.pt...\n",
            "/ [0/9 files][    0.0 B/  3.3 GiB]   0% Done                                    \r/ [0/9 files][    0.0 B/  3.3 GiB]   0% Done                                    \r==> NOTE: You are downloading one or more large file(s), which would\n",
            "run significantly faster if you enabled sliced object downloads. This\n",
            "feature is enabled by default but requires that compiled crcmod be\n",
            "installed (see \"gsutil help crcmod\").\n",
            "\n",
            "Copying gs://indaba-data/train/train_wt.pt...\n",
            "Copying gs://indaba-data/train/train.csv...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Imports and moving to working directory\n",
        "import torch \n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# move to data folder\n",
        "%cd indaba-data"
      ],
      "metadata": {
        "id": "Jvd8ERpgTvji",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c0fe3980-4427-4c93-972c-d9c889ee1081"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/indaba-data/indaba-data\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load Embedding tensors & Traing csv\n",
        "# Embeddings were calculated using the ESM 650M pretrained model \n",
        "# Tensor shape of embedded data:  [data_len,1280] \n",
        "# There are no sequences in the Embedding tensors as we've performed an average of it (torch.mean(embed, dim=1))\n",
        "# More details in https://huggingface.co/facebook/esm2_t33_650M_UR50D\n",
        "\n",
        "\n",
        "\n",
        "wt_emb = torch.load(\"train/train_wt.pt\")\n",
        "mut_emb = torch.load(\"train/train_mut.pt\")\n",
        "df = pd.read_csv(\"train/train.csv\")"
      ],
      "metadata": {
        "id": "36ZgVoj5odV4"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# [Recommended] Split data into train and validation \n",
        "\n",
        "\n",
        "\n",
        "# Load embedding tensors\n",
        "train_mut_embeddings = torch.load('train/train_mut.pt')\n",
        "train_wt_embeddings = torch.load('train/train_wt.pt')\n",
        "\n",
        "# Load training CSV\n",
        "train_data = pd.read_csv('train/train.csv')\n",
        "\n",
        "# Splitting the data into train and validation sets\n",
        "train_data, val_data = train_test_split(train_data, test_size=0.2, random_state=42)\n",
        "\n",
        "# Printing the number of samples in each set\n",
        "print(\"Number of samples in the training set:\", len(train_data))\n",
        "print(\"Number of samples in the validation set:\", len(val_data))\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#TODO"
      ],
      "metadata": {
        "id": "zr0Njii0pRvN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "12415d58-2cbd-44be-c794-c691c6c98177"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of samples in the training set: 271822\n",
            "Number of samples in the validation set: 67956\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Building the dataset class\n",
        "class EmbeddingDataset(torch.utils.data.Dataset):\n",
        "  def __init__(self,mut_pt, wt_pt, data_df):\n",
        "    self.pt_mut = mut_pt\n",
        "    self.pt_wt = wt_pt\n",
        "    self.df = data_df\n",
        "  \n",
        "  def __len__(self):\n",
        "      return self.pt_mut.shape[0]\n",
        "\n",
        "  def __getitem__(self, index):\n",
        "    o1=self.pt_mut[index,:]\n",
        "    o2=self.pt_wt[index,:]\n",
        "    if \"ddg\" in self.df:\n",
        "      df_out=torch.Tensor([self.df[\"ddg\"][index]])\n",
        "    else:\n",
        "      df_out=torch.Tensor([self.df[\"ID\"][index]])\n",
        "    return  self.pt_mut[index,:],self.pt_wt[index,:],df_out "
      ],
      "metadata": {
        "id": "BEEH-ZWJgdUv"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# creating training dataset and dataloader\n",
        "train_dataset = EmbeddingDataset(wt_emb, mut_emb, df)\n",
        "# preparing a dataloader for the training\n",
        "train_dataloader = torch.utils.data.dataloader.DataLoader(\n",
        "        train_dataset,\n",
        "        batch_size=32,\n",
        "        shuffle=False,\n",
        "        num_workers=2,\n",
        "    )\n",
        "# [Recommended] Use Data validation loader too\n"
      ],
      "metadata": {
        "id": "RrRVCI8siRfF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Building a simple pytorch model\n",
        "# A dummy model (2-param) that demonstrates the usage of the dataset\n",
        "\n",
        "class StabilityModel(torch.nn.Module):\n",
        "  def __init__(self):\n",
        "    super(StabilityModel, self).__init__()\n",
        "    self.lin = torch.nn.Linear(1,1)\n",
        "\n",
        "  def forward(self, x, y):\n",
        "    # run the forward pass\n",
        "    # output should be the stability estimation [batch,estim]\n",
        "    return self.lin(torch.mean(x-y,dim=1).reshape(-1,1)) "
      ],
      "metadata": {
        "id": "9NuUlQRK8gHF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Example of training script\n",
        "device = torch.device(\"cuda\")\n",
        "model =  StabilityModel().to(device)\n",
        "optimizer = torch.optim.Adadelta(model.parameters(), lr=0.0001)\n",
        "criterion = torch.nn.MSELoss()\n",
        "epoch_loss = 0\n",
        "for i in range(1):\n",
        "  epoch_loss = 0\n",
        "  for batch_idx, (data_mut,data_wt , target) in tqdm(enumerate(train_dataloader)):\n",
        "      # extract input from datallader\n",
        "      x1 = data_wt.to(device)\n",
        "      x2 = data_mut.to(device)\n",
        "      y = target.to(device)\n",
        "      # make prediction\n",
        "      y_pred = model(x1,x2)\n",
        "      # calculate loss and run optimizer\n",
        "      loss = torch.sqrt(criterion(y, y_pred))\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "      epoch_loss += loss\n",
        "  print(\"epoch_\",i,\" = \", epoch_loss/len(train_dataloader))\n",
        "  # [Recommended] Save trained models to select best checkpoint for prediction (or add prediction in the epochs loop)"
      ],
      "metadata": {
        "id": "ndGfhMrjlmUE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Prediction & submission"
      ],
      "metadata": {
        "id": "9GDKutS_nKOT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# load embedding tensors & traing csv\n",
        "wt_test_emb = torch.load(\"test/test_wt.pt\")\n",
        "mut_test_emb = torch.load(\"test/test_mut.pt\")\n",
        "df_test = pd.read_csv(\"test/test.csv\")"
      ],
      "metadata": {
        "id": "ave_DDMp8fo9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# creating testing dataset and loading the embedding\n",
        "test_dataset = EmbeddingDataset(wt_test_emb,mut_test_emb,df_test)\n",
        "# preparing a dataloader for the testing\n",
        "test_dataloader = torch.utils.data.dataloader.DataLoader(\n",
        "        test_dataset,\n",
        "        batch_size=32,\n",
        "        shuffle=False,\n",
        "        num_workers=2,\n",
        "    )"
      ],
      "metadata": {
        "id": "9Xmav2yhm_Di"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_result = pd.DataFrame()\n",
        "with torch.no_grad():\n",
        "  for batch_idx, (data_mut,data_wt , target) in tqdm(enumerate(test_dataloader)):\n",
        "    x1 = data_wt.to(device)\n",
        "    x2 = data_mut.to(device)\n",
        "    id = target.to(device)\n",
        "    # make prediction\n",
        "    y_pred = model(x1,x2)\n",
        "    df_result = pd.concat([df_result, pd.DataFrame({\"ID\":id.squeeze().cpu().numpy().astype(int) , \"ddg\" : y_pred.squeeze().cpu().numpy()})])"
      ],
      "metadata": {
        "id": "DiylsXvjqOul"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_result.to_csv(\"submission.csv\",index=False)"
      ],
      "metadata": {
        "id": "FPm-a2USexgw"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}