{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/FaroukChalghoumi/IndabaX/blob/Transformers/NewTrainingLoop.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Download data from GCP bucket\n",
        "import sys\n",
        "\n",
        "if 'google.colab' in sys.modules:\n",
        "  !gsutil -m cp -r gs://indaba-data .\n",
        "else:\n",
        "  !mkdir -p indaba-data/train\n",
        "  !wget -P indaba-data/train https://storage.googleapis.com/indaba-data/train/train.csv --continue\n",
        "  !wget -P indaba-data/train https://storage.googleapis.com/indaba-data/train/train_mut.pt --continue\n",
        "  !wget -P indaba-data/train https://storage.googleapis.com/indaba-data/train/train_wt.pt --continue\n",
        "\n",
        "  !mkdir -p indaba-data/test\n",
        "  !wget -P indaba-data/test https://storage.googleapis.com/indaba-data/test/test.csv --continue\n",
        "  !wget -P indaba-data/test https://storage.googleapis.com/indaba-data/test/test_mut.pt --continue\n",
        "  !wget -P indaba-data/test https://storage.googleapis.com/indaba-data/test/test_wt.pt --continue"
      ],
      "metadata": {
        "id": "DEk5q0rhjBWZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7311a66a-9d30-4797-bf90-24f236f48c83"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Copying gs://indaba-data/test/test_mut.pt...\n",
            "/ [0 files][    0.0 B/  9.3 MiB]                                                \rCopying gs://indaba-data/README.txt...\n",
            "Copying gs://indaba-data/test/test.csv...\n",
            "/ [0 files][    0.0 B/  9.3 MiB]                                                \r/ [0 files][    0.0 B/  9.6 MiB]                                                \rCopying gs://indaba-data/test/test_wt.pt...\n",
            "Copying gs://indaba-data/train/train_mut.pt...\n",
            "/ [0/9 files][    0.0 B/  3.3 GiB]   0% Done                                    \r/ [0/9 files][    0.0 B/  3.3 GiB]   0% Done                                    \rCopying gs://indaba-data/train/train.csv...\n",
            "/ [0/9 files][    0.0 B/  3.3 GiB]   0% Done                                    \rCopying gs://indaba-data/train/train_wt.pt...\n",
            "/ [0/9 files][    0.0 B/  3.3 GiB]   0% Done                                    \r==> NOTE: You are downloading one or more large file(s), which would\n",
            "run significantly faster if you enabled sliced object downloads. This\n",
            "feature is enabled by default but requires that compiled crcmod be\n",
            "installed (see \"gsutil help crcmod\").\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Imports and moving to working directory\n",
        "import torch \n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "from sklearn.model_selection import train_test_split\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "# move to data folder\n",
        "%cd /content/indaba-data"
      ],
      "metadata": {
        "id": "Jvd8ERpgTvji",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9ce31066-0983-443c-f803-55014756e9aa"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/indaba-data\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load Embedding tensors & Traing csv\n",
        "# Embeddings were calculated using the ESM 650M pretrained model \n",
        "# Tensor shape of embedded data:  [data_len,1280] \n",
        "# There are no sequences in the Embedding tensors as we've performed an average of it (torch.mean(embed, dim=1))\n",
        "# More details in https://huggingface.co/facebook/esm2_t33_650M_UR50D\n",
        "\n",
        "wt_emb = torch.load(\"train/train_wt.pt\")\n",
        "mut_emb = torch.load(\"train/train_mut.pt\")\n",
        "df = pd.read_csv(\"train/train.csv\")"
      ],
      "metadata": {
        "id": "36ZgVoj5odV4"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# [Recommended] Split data into train and validation \n",
        "#TODO\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Load the dataset\n",
        "train_data = pd.read_csv('train/train.csv')\n",
        "\n",
        "# 1. Extract relevant columns: ID, ddg\n",
        "relevant_columns = ['ID', 'pdb_id', 'mutation', 'wt_seq', 'mut_seq', 'ddg']\n",
        "train_data = train_data[relevant_columns]\n",
        "\n",
        "# 2. Convert amino acid sequences (wt_seq and mut_seq) to numerical representations\n",
        "# Define the amino acid alphabet\n",
        "amino_acids = 'ACDEFGHIKLMNPQRSTVWY'\n",
        "\n",
        "# Convert amino acid sequence to numerical representation\n",
        "def seq_to_numerical(sequence):\n",
        "    return np.array([amino_acids.index(aa) for aa in sequence], dtype=np.int64)\n",
        "\n",
        "# Apply the conversion to wt_seq and mut_seq columns\n",
        "train_data['wt_seq'] = train_data['wt_seq'].apply(seq_to_numerical)\n",
        "train_data['mut_seq'] = train_data['mut_seq'].apply(seq_to_numerical)\n",
        "\n",
        "# 3. Normalize the input data if necessary\n",
        "# Assuming you want to normalize the ddg column, you can use StandardScaler from sklearn\n",
        "scaler = StandardScaler()\n",
        "train_data['ddg'] = scaler.fit_transform(train_data[['ddg']].astype(np.float64))\n",
        "\n",
        "# 4. Handle missing values, if any\n",
        "# Assuming you want to drop rows with missing values\n",
        "train_data.dropna(inplace=True)\n",
        "\n",
        "# 5. Any other preprocessing steps specific to your use case\n",
        "\n",
        "# Convert data into PyTorch tensors\n",
        "#train_mut_tensors = torch.tensor(train_data['mutation'].values)\n",
        "# Convert data into PyTorch tensors\n",
        "\n",
        "# Convert data into PyTorch tensors\n",
        "train_wt_tensors = [torch.tensor(seq) for seq in train_data['wt_seq']]\n",
        "train_ddg_tensors = torch.tensor(train_data['ddg'].values, dtype=torch.float32)\n",
        "\n",
        "# Pad sequences to ensure equal length\n",
        "train_wt_tensors = pad_sequence(train_wt_tensors, batch_first=True)\n",
        "train_ddg_tensors = train_ddg_tensors.unsqueeze(1)\n",
        "\n",
        "# Split the data into train and validation sets\n",
        "train_wt_tensors, val_wt_tensors, train_ddg_tensors, val_ddg_tensors = train_test_split(\n",
        "    train_wt_tensors, train_ddg_tensors, test_size=0.2, random_state=42)\n",
        "\n",
        "# Create DataLoader instances for training and validation sets\n",
        "train_dataset = torch.utils.data.TensorDataset(train_wt_tensors, train_ddg_tensors)\n",
        "val_dataset = torch.utils.data.TensorDataset(val_wt_tensors, val_ddg_tensors)\n",
        "\n",
        "train_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "val_dataloader = torch.utils.data.DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
        "\n",
        "# Now you have train and validation dataloaders with the preprocessed data\n",
        "# You can pass these dataloaders to your model for training and evaluation\n",
        "\n",
        "# Example usage:\n",
        "# for epoch in range(num_epochs):\n",
        "#     # Training loop\n",
        "#     for batch_mut, batch_wt, batch_ddg in train_dataloader:\n",
        "#         # Training steps\n",
        "\n",
        "#     # Validation loop\n",
        "#     for batch_mut, batch_wt, batch_ddg in val_dataloader:\n",
        "#         # Validation steps\n",
        "\n",
        "\n",
        "\n",
        "# # Load embedding tensors\n",
        "# train_mut_embeddings = torch.load('train/train_mut.pt')\n",
        "# train_wt_embeddings = torch.load('train/train_wt.pt')\n",
        "\n",
        "# # Load training CSV\n",
        "# train_data = pd.read_csv('train/train.csv')\n",
        "\n",
        "# # Splitting the data into train and validation sets\n",
        "# train_data, val_data = train_test_split(train_data, test_size=0.2, random_state=42)\n",
        "\n",
        "# # Printing the number of samples in each set\n",
        "# print(\"Number of samples in the training set:\", len(train_data))\n",
        "# print(\"Number of samples in the validation set:\", len(val_data))"
      ],
      "metadata": {
        "id": "zr0Njii0pRvN"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Building the dataset class\n",
        "class EmbeddingDataset(torch.utils.data.Dataset):\n",
        "  def __init__(self,mut_pt, wt_pt, data_df):\n",
        "    self.pt_mut = mut_pt\n",
        "    self.pt_wt = wt_pt\n",
        "    self.df = data_df\n",
        "  \n",
        "  def __len__(self):\n",
        "      return self.pt_mut.shape[0]\n",
        "\n",
        "  def __getitem__(self, index):\n",
        "    o1=self.pt_mut[index,:]\n",
        "    o2=self.pt_wt[index,:]\n",
        "    if \"ddg\" in self.df:\n",
        "      df_out=torch.Tensor([self.df[\"ddg\"][index]])\n",
        "    else:\n",
        "      df_out=torch.Tensor([self.df[\"ID\"][index]])\n",
        "    return  self.pt_mut[index,:],self.pt_wt[index,:],df_out "
      ],
      "metadata": {
        "id": "BEEH-ZWJgdUv"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# creating training dataset and dataloader\n",
        "train_dataset = EmbeddingDataset(wt_emb, mut_emb, df)\n",
        "# preparing a dataloader for the training\n",
        "train_dataloader = torch.utils.data.dataloader.DataLoader(\n",
        "        train_dataset,\n",
        "        batch_size=64,\n",
        "        shuffle=True,\n",
        "        num_workers=2,\n",
        "    )\n",
        "# [Recommended] Use Data validation loader too\n"
      ],
      "metadata": {
        "id": "RrRVCI8siRfF"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # Building a simple pytorch model\n",
        "# # A dummy model (2-param) that demonstrates the usage of the dataset\n",
        "\n",
        "# class StabilityModel(torch.nn.Module):\n",
        "#   def __init__(self):\n",
        "#     super(StabilityModel, self).__init__()\n",
        "#     self.lin = torch.nn.Linear(1,1)\n",
        "\n",
        "#   def forward(self, x, y):\n",
        "#     # run the forward pass\n",
        "#     # output should be the stability estimation [batch,estim]\n",
        "#     return self.lin(torch.mean(x-y,dim=1).reshape(-1,1)) \n",
        "\n",
        "# class ProteinModel(nn.Module):\n",
        "#     def __init__(self, input_dim):\n",
        "#         super(ProteinModel, self).__init__()\n",
        "#         self.fc1 = nn.Linear(input_dim * 2, 256)\n",
        "#         self.fc2 = nn.Linear(256, 128)\n",
        "#         self.fc3 = nn.Linear(128, 1)\n",
        "#         self.relu = nn.ReLU()\n",
        "        \n",
        "#     def forward(self, mut_emb, wt_emb):\n",
        "#         x = torch.cat((mut_emb, wt_emb), dim=1)\n",
        "#         x = self.fc1(x)\n",
        "#         x = self.relu(x)\n",
        "#         x = self.fc2(x)\n",
        "#         x = self.relu(x)\n",
        "#         output = self.fc3(x)\n",
        "#         return output.squeeze(1)\n",
        "\n",
        "class ProteinModel(nn.Module):\n",
        "    def __init__(self, input_dim):\n",
        "        super(ProteinModel, self).__init__()\n",
        "        self.fc1 = nn.Linear(input_dim * 2, 512)\n",
        "        self.fc2 = nn.Linear(512, 256)\n",
        "        self.fc3 = nn.Linear(256, 128)\n",
        "        self.fc4 = nn.Linear(128, 64)\n",
        "        self.fc5 = nn.Linear(64, 1)\n",
        "        self.relu = nn.ReLU()\n",
        "\n",
        "    def forward(self, mut_emb, wt_emb):\n",
        "        x = torch.cat((mut_emb, wt_emb), dim=1)\n",
        "        x = self.fc1(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.fc2(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.fc3(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.fc4(x)\n",
        "        x = self.relu(x)\n",
        "        output = self.fc5(x)\n",
        "        return output.squeeze(1)\n",
        "\n",
        "\n",
        "# Instantiate the model\n",
        "input_dim = 1280  # Update with the actual input dimension\n",
        "model = ProteinModel(input_dim)\n",
        "\n",
        "# Set the model to evaluation mode\n",
        "model.eval()\n"
      ],
      "metadata": {
        "id": "9NuUlQRK8gHF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1f45e51d-dfcb-418b-d3fb-a90a7ed129fc"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ProteinModel(\n",
              "  (fc1): Linear(in_features=2560, out_features=512, bias=True)\n",
              "  (fc2): Linear(in_features=512, out_features=256, bias=True)\n",
              "  (fc3): Linear(in_features=256, out_features=128, bias=True)\n",
              "  (fc4): Linear(in_features=128, out_features=64, bias=True)\n",
              "  (fc5): Linear(in_features=64, out_features=1, bias=True)\n",
              "  (relu): ReLU()\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# # Example of training script\n",
        "# device = torch.device(\"cuda\")\n",
        "# model =  StabilityModel().to(device)\n",
        "# optimizer = torch.optim.Adadelta(model.parameters(), lr=0.0001)\n",
        "# criterion = torch.nn.MSELoss()\n",
        "# epoch_loss = 0\n",
        "# for i in range(1):\n",
        "#   epoch_loss = 0\n",
        "#   for batch_idx, (data_mut,data_wt , target) in tqdm(enumerate(train_dataloader)):\n",
        "#       # extract input from datallader\n",
        "#       x1 = data_wt.to(device)\n",
        "#       x2 = data_mut.to(device)\n",
        "#       y = target.to(device)\n",
        "#       # make prediction\n",
        "#       y_pred = model(x1,x2)\n",
        "#       # calculate loss and run optimizer\n",
        "#       loss = torch.sqrt(criterion(y, y_pred))\n",
        "#       loss.backward()\n",
        "#       optimizer.step()\n",
        "#       epoch_loss += loss\n",
        "#   print(\"epoch_\",i,\" = \", epoch_loss/len(train_dataloader))\n",
        "# #   # [Recommended] Save trained models to select best checkpoint for prediction (or add prediction in the epochs loop)\n",
        "\n",
        "# # Set device\n",
        "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "\n",
        "\n",
        "# # Define hyperparameters\n",
        "# learning_rate = 0.001\n",
        "# batch_size = 32\n",
        "# num_epochs = 10\n",
        "\n",
        "# # Create instances of your model and move them to the device\n",
        "# model = YourModel().to(device)\n",
        "\n",
        "# # Define the loss function\n",
        "# loss_fn = nn.MSELoss()\n",
        "\n",
        "# # Define the optimizer\n",
        "# learning_rate = 0.001\n",
        "\n",
        "# model = ProteinModel(input_dim)  # Instantiate the model with the appropriate input dimension\n",
        "# optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "\n",
        "# # Create DataLoader instances for your training and validation datasets\n",
        "# train_dataset = EmbeddingDataset(train_mut_embeddings, train_wt_embeddings, train_data)\n",
        "# train_loader = tqdm(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "# val_dataset = EmbeddingDataset(train_mut_embeddings, train_wt_embeddings, val_data)\n",
        "# val_loader = DataLoader(val_dataset, batch_size=batch_size)\n",
        "\n",
        "# # Training loop\n",
        "# for epoch in range(num_epochs):\n",
        "#     # Set model to training mode\n",
        "#     model.train()\n",
        "\n",
        "#     for batch in train_loader:\n",
        "#         # Move batch to device\n",
        "#         batch = [item.to(device) for item in batch]\n",
        "\n",
        "#         # Forward pass\n",
        "#         output = model(batch[0], batch[1])\n",
        "\n",
        "#         # Compute loss\n",
        "#         loss = loss_fn(output, batch[2])\n",
        "\n",
        "#         # Backward pass and optimization\n",
        "#         optimizer.zero_grad()\n",
        "#         loss.backward()\n",
        "#         optimizer.step()\n",
        "\n",
        "#     # Set model to evaluation mode\n",
        "#     model.eval()\n",
        "\n",
        "#     # Validation\n",
        "#     with torch.no_grad():\n",
        "#         val_loss = 0.0\n",
        "#         num_val_samples = 0\n",
        "\n",
        "#         for batch in val_loader:\n",
        "#             # Move batch to device\n",
        "#             batch = [item.to(device) for item in batch]\n",
        "\n",
        "#             # Forward pass\n",
        "#             output = model(batch[0], batch[1])\n",
        "\n",
        "#             # Compute loss\n",
        "#             val_loss += loss_fn(output, batch[2]).item() * batch[0].size(0)\n",
        "#             num_val_samples += batch[0].size(0)\n",
        "\n",
        "#         val_loss /= num_val_samples\n",
        "\n",
        "#     # Print training and validation loss for each epoch\n",
        "#     print(f\"Epoch {epoch+1}/{num_epochs}, Training Loss: {loss.item():.4f}, Validation Loss: {val_loss:.4f}\")\n",
        "\n",
        "# # Save the trained model\n",
        "# torch.save(model.state_dict(), \"trained_model.pt\")\n",
        "\n",
        "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# model = ProteinModel(input_dim=1280).to(device)\n",
        "# optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "# criterion = torch.nn.MSELoss()\n",
        "\n",
        "# num_epochs = 10\n",
        "\n",
        "# train_losses = []\n",
        "# val_losses = []\n",
        "\n",
        "# for epoch in range(num_epochs):\n",
        "#     model.train()\n",
        "#     train_loss = 0.0\n",
        "    \n",
        "#     for batch_idx, (data_mut, data_wt, target) in tqdm(enumerate(train_dataloader), total=len(train_dataloader), desc=f\"Epoch {epoch+1}\"):\n",
        "#         data_mut = data_mut.to(device)\n",
        "#         data_wt = data_wt.to(device)\n",
        "#         target = target.to(device)\n",
        "        \n",
        "#         optimizer.zero_grad()\n",
        "        \n",
        "#         # Forward pass\n",
        "#         output = model(data_mut, data_wt)\n",
        "        \n",
        "#         # Compute loss\n",
        "#         loss = torch.sqrt(criterion(output, target))\n",
        "        \n",
        "#         # Backward pass\n",
        "#         loss.backward()\n",
        "#         optimizer.step()\n",
        "        \n",
        "#         train_loss += loss.item()\n",
        "    \n",
        "#     avg_train_loss = train_loss / len(train_dataloader)\n",
        "#     train_losses.append(avg_train_loss)\n",
        "    \n",
        "#     model.eval()\n",
        "#     val_loss = 0.0\n",
        "    \n",
        "#     with torch.no_grad():\n",
        "#         for batch_idx, (data_mut, data_wt, target) in tqdm(enumerate(test_dataloader), total=len(test_dataloader), desc=\"Validation\"):\n",
        "#             data_mut = data_mut.to(device)\n",
        "#             data_wt = data_wt.to(device)\n",
        "#             target = target.to(device)\n",
        "            \n",
        "#             output = model(data_mut, data_wt)\n",
        "#             loss = torch.sqrt(criterion(output, target))\n",
        "#             val_loss += loss.item()\n",
        "        \n",
        "#     avg_val_loss = val_loss / len(test_dataloader)\n",
        "#     val_losses.append(avg_val_loss)\n",
        "    \n",
        "#     print(f\"Epoch {epoch+1}: Train Loss = {avg_train_loss}, Validation Loss = {avg_val_loss}\")\n",
        "\n",
        "# # Save the trained model\n",
        "# torch.save(model.state_dict(), \"trained_model.pth\")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#####################################################################\n",
        "# # Set device\n",
        "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# # Initialize your improved model\n",
        "# model = ProteinModel(input_dim=1280).to(device)\n",
        "\n",
        "# # Define loss function (MSE)\n",
        "# criterion = nn.MSELoss()\n",
        "\n",
        "# # Define optimizer with a lower learning rate\n",
        "# optimizer = torch.optim.Adam(model.parameters(), lr=0.0001)\n",
        "\n",
        "# # Training loop\n",
        "# num_epochs = 20  # Increase the number of epochs for better training\n",
        "# best_val_loss = float('inf')\n",
        "# skip_epochs = 0\n",
        "# prev_train_loss = None\n",
        "# prev_val_loss = None\n",
        "\n",
        "# for epoch in range(num_epochs):\n",
        "#     train_loss = 0.0\n",
        "#     val_loss = 0.0\n",
        "    \n",
        "#     # Training phase\n",
        "#     model.train()\n",
        "#     for data_mut, data_wt, target in train_dataloader:\n",
        "#         data_mut = data_mut.to(device)\n",
        "#         data_wt = data_wt.to(device)\n",
        "#         target = target.to(device)\n",
        "        \n",
        "#         optimizer.zero_grad()\n",
        "#         output = model(data_mut, data_wt)\n",
        "#         loss = criterion(output, target)\n",
        "#         loss.backward()\n",
        "#         optimizer.step()\n",
        "        \n",
        "#         train_loss += loss.item() * data_mut.size(0)\n",
        "    \n",
        "#     train_loss /= len(train_dataloader.dataset)\n",
        "    \n",
        "#     # Validation phase\n",
        "#     model.eval()\n",
        "#     with torch.no_grad():\n",
        "#         for data_mut, data_wt, target in train_dataloader:\n",
        "#             data_mut = data_mut.to(device)\n",
        "#             data_wt = data_wt.to(device)\n",
        "#             target = target.to(device)\n",
        "            \n",
        "#             output = model(data_mut, data_wt)\n",
        "#             loss = criterion(output, target)\n",
        "            \n",
        "#             val_loss += loss.item() * data_mut.size(0)\n",
        "    \n",
        "#     val_loss /= len(train_dataloader.dataset)\n",
        "    \n",
        "#     print(f\"Epoch {epoch+1}: Training Loss = {train_loss}, Validation Loss = {val_loss}\")\n",
        "    \n",
        "#     # Check if training loss and validation loss are the same as the previous epoch\n",
        "#     if train_loss == prev_train_loss and val_loss == prev_val_loss:\n",
        "#         skip_epochs += 1\n",
        "#         if skip_epochs >= 3:\n",
        "#             print(\"Training loss and validation loss are not improving. Stopping training.\")\n",
        "#             break\n",
        "#     else:\n",
        "#         skip_epochs = 0\n",
        "    \n",
        "#     prev_train_loss = train_loss\n",
        "#     prev_val_loss = val_loss\n",
        "    \n",
        "#     # Save the model if validation loss is improved\n",
        "#     if val_loss < best_val_loss:\n",
        "#         best_val_loss = val_loss\n",
        "#         torch.save(model.state_dict(), \"best_model.pt\")\n",
        "\n",
        "\n",
        "\n",
        "# Instantiate the model\n",
        "input_dim = 1280  # Update with the actual input dimension\n",
        "model = ProteinModel(input_dim).to(device)\n",
        "\n",
        "# Define the loss function\n",
        "criterion = nn.MSELoss()\n",
        "\n",
        "# Define the optimizer\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "# Define the training loop\n",
        "num_epochs = 10\n",
        "best_val_loss = float('inf')\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    train_loss = 0.0\n",
        "    val_loss = 0.0\n",
        "    \n",
        "    # Training phase\n",
        "    model.train()\n",
        "    for data_mut, data_wt, target in train_dataloader:\n",
        "        data_mut = data_mut.to(device)\n",
        "        data_wt = data_wt.to(device)\n",
        "        target = target.to(device)\n",
        "        \n",
        "        optimizer.zero_grad()\n",
        "        output = model(data_mut, data_wt)\n",
        "        loss = criterion(output, target)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        \n",
        "        train_loss += loss.item() * data_mut.size(0)\n",
        "    \n",
        "    train_loss /= len(train_dataloader.dataset)\n",
        "    \n",
        "    # Validation phase\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        for data_mut, data_wt, target in val_dataloader:\n",
        "            data_mut = data_mut.to(device)\n",
        "            data_wt = data_wt.to(device)\n",
        "            target = target.to(device)\n",
        "            \n",
        "            output = model(data_mut, data_wt)\n",
        "            loss = criterion(output, target)\n",
        "            \n",
        "            val_loss += loss.item() * data_mut.size(0)\n",
        "    \n",
        "    val_loss /= len(val_dataloader.dataset)\n",
        "    \n",
        "    print(f\"Epoch {epoch+1}: Training Loss = {train_loss}, Validation Loss = {val_loss}\")\n",
        "    \n",
        "    # Save the model if validation loss is improved\n",
        "    if val_loss < best_val_loss:\n",
        "        best_val_loss = val_loss\n",
        "        torch.save(model.state_dict(), \"best_model.pt\")\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "ndGfhMrjlmUE",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 324
        },
        "outputId": "169b2026-388a-4258-fd31-54eaf468db68"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/loss.py:536: UserWarning: Using a target size (torch.Size([64, 1])) that is different to the input size (torch.Size([64])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
            "  return F.mse_loss(input, target, reduction=self.reduction)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/loss.py:536: UserWarning: Using a target size (torch.Size([2, 1])) that is different to the input size (torch.Size([2])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
            "  return F.mse_loss(input, target, reduction=self.reduction)\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-16-6e90e77fcf43>\u001b[0m in \u001b[0;36m<cell line: 262>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    284\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    285\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 286\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mdata_mut\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_wt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mval_dataloader\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    287\u001b[0m             \u001b[0mdata_mut\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata_mut\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    288\u001b[0m             \u001b[0mdata_wt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata_wt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: not enough values to unpack (expected 3, got 2)"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the learning rate scheduler\n",
        "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', patience=2)\n",
        "\n",
        "# Define the training loop\n",
        "num_epochs = 10\n",
        "batch_size = 64\n",
        "accumulation_steps = 8\n",
        "best_val_loss = float('inf')\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "train_loss_history = []\n",
        "val_loss_history = []\n",
        "\n",
        "model.train()\n",
        "for epoch in range(num_epochs):\n",
        "    train_loss = 0.0\n",
        "    val_loss = 0.0\n",
        "    \n",
        "    for batch_idx, (data_mut, data_wt, target) in enumerate(train_dataloader):\n",
        "        data_mut = data_mut.to(device)\n",
        "        data_wt = data_wt.to(device)\n",
        "        target = target.to(device)\n",
        "        \n",
        "        optimizer.zero_grad()\n",
        "        output = model(data_mut, data_wt)\n",
        "        loss = criterion(output, target)\n",
        "        loss.backward()\n",
        "        \n",
        "        if (batch_idx + 1) % accumulation_steps == 0:\n",
        "            optimizer.step()\n",
        "            optimizer.zero_grad()\n",
        "        \n",
        "        train_loss += loss.item() * data_mut.size(0)\n",
        "    \n",
        "    train_loss /=len(train_dataloader.dataset)\n",
        "    # Validation phase\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    for data_mut, data_wt, target in val_dataloader:\n",
        "        data_mut = data_mut.to(device)\n",
        "        data_wt = data_wt.to(device)\n",
        "        target = target.to(device)\n",
        "        \n",
        "        output = model(data_mut, data_wt)\n",
        "        loss = criterion(output, target)\n",
        "        \n",
        "        val_loss += loss.item() * data_mut.size(0)\n",
        "\n",
        "        val_loss /= len(val_dataloader.dataset)\n",
        "\n",
        "        # Update learning rate scheduler\n",
        "        scheduler.step(val_loss)\n",
        "\n",
        "        # Save the model if validation loss is improved\n",
        "        if val_loss < best_val_loss:\n",
        "            best_val_loss = val_loss\n",
        "            torch.save(model.state_dict(), \"best_model.pt\")\n",
        "\n",
        "        # Print epoch statistics\n",
        "        print(f\"Epoch {epoch+1}: Training Loss = {train_loss:.4f}, Validation Loss = {val_loss:.4f}\")\n",
        "\n",
        "        # Keep track of loss history\n",
        "        train_loss_history.append(train_loss)\n",
        "        val_loss_history.append(val_loss)\n",
        "\n",
        "        # Check early stopping condition\n",
        "        if epoch > 0 and val_loss_history[-1] >= val_loss_history[-2]:\n",
        "            print(\"Validation loss did not improve. Stopping early.\")\n",
        "        break \n",
        "\n",
        "        plt.plot(train_loss_history, label='Training Loss')\n",
        "        plt.plot(val_loss_history, label='Validation Loss')\n",
        "        plt.xlabel('Epoch')\n",
        "        plt.ylabel('Loss')\n",
        "        plt.legend()\n",
        "        plt.show()\n"
      ],
      "metadata": {
        "id": "Rt3W_wI0kUvt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Prediction & submission"
      ],
      "metadata": {
        "id": "9GDKutS_nKOT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "df_result = pd.DataFrame()\n",
        "with torch.no_grad():\n",
        "    for batch_idx, (data_mut, data_wt, target) in tqdm(enumerate(train_dataloader)):\n",
        "        data_mut = data_mut.to(device)\n",
        "        data_wt = data_wt.to(device)\n",
        "        id = target.to(device)\n",
        "        # Make prediction\n",
        "        y_pred = model(data_mut, data_wt)\n",
        "        df_batch = pd.DataFrame({\"ID\": id.squeeze().cpu().numpy().astype(int), \"DDG\": y_pred.squeeze().cpu().numpy()})\n",
        "        df_result = pd.concat([df_result, df_batch])\n",
        "\n",
        "# Save the DataFrame to a CSV file\n",
        "df_result.to_csv(\"predictionsnewwwwww.csv\", index=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JMCrNc8t5rAX",
        "outputId": "d6b057a5-530a-41a7-fad3-b5a6386f5796"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "5310it [01:12, 73.60it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# load embedding tensors & traing csv\n",
        "wt_test_emb = torch.load(\"test/test_wt.pt\")\n",
        "mut_test_emb = torch.load(\"test/test_mut.pt\")\n",
        "df_test = pd.read_csv(\"test/test.csv\")"
      ],
      "metadata": {
        "id": "ave_DDMp8fo9"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# creating testing dataset and loading the embedding\n",
        "test_dataset = EmbeddingDataset(wt_test_emb,mut_test_emb,df_test)\n",
        "# preparing a dataloader for the testing\n",
        "test_dataloader = torch.utils.data.dataloader.DataLoader(\n",
        "        test_dataset,\n",
        "        batch_size=32,\n",
        "        shuffle=False,\n",
        "        num_workers=2,\n",
        "    )"
      ],
      "metadata": {
        "id": "9Xmav2yhm_Di"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_result = pd.DataFrame()\n",
        "with torch.no_grad():\n",
        "  for batch_idx, (data_mut,data_wt , target) in tqdm(enumerate(test_dataloader)):\n",
        "    x1 = data_wt.to(device)\n",
        "    x2 = data_mut.to(device)\n",
        "    id = target.to(device)\n",
        "    # make prediction\n",
        "    y_pred = model(x1,x2)\n",
        "    df_result = pd.concat([df_result, pd.DataFrame({\"ID\":id.squeeze().cpu().numpy().astype(int) , \"ddg\" : y_pred.squeeze().cpu().numpy()})])"
      ],
      "metadata": {
        "id": "DiylsXvjqOul",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e2731fe8-d65b-40b8-cf26-88d06dbfbaa8"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "60it [00:00, 88.32it/s] \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_result.to_csv(\"submission.csv\",index=False)"
      ],
      "metadata": {
        "id": "FPm-a2USexgw"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "7PGmPkRmezal"
      },
      "execution_count": 12,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}