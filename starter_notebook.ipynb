{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/FaroukChalghoumi/IndabaX/blob/Transformers/starter_notebook.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Download data from GCP bucket\n",
        "import sys\n",
        "\n",
        "if 'google.colab' in sys.modules:\n",
        "  !gsutil -m cp -r gs://indaba-data .\n",
        "else:\n",
        "  !mkdir -p indaba-data/train\n",
        "  !wget -P indaba-data/train https://storage.googleapis.com/indaba-data/train/train.csv --continue\n",
        "  !wget -P indaba-data/train https://storage.googleapis.com/indaba-data/train/train_mut.pt --continue\n",
        "  !wget -P indaba-data/train https://storage.googleapis.com/indaba-data/train/train_wt.pt --continue\n",
        "\n",
        "  !mkdir -p indaba-data/test\n",
        "  !wget -P indaba-data/test https://storage.googleapis.com/indaba-data/test/test.csv --continue\n",
        "  !wget -P indaba-data/test https://storage.googleapis.com/indaba-data/test/test_mut.pt --continue\n",
        "  !wget -P indaba-data/test https://storage.googleapis.com/indaba-data/test/test_wt.pt --continue"
      ],
      "metadata": {
        "id": "DEk5q0rhjBWZ",
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d6f1a7a9-48bb-4ae5-c1a3-4ac0b3150925"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Copying gs://indaba-data/test/test.csv...\n",
            "/ [0/9 files][    0.0 B/  3.3 GiB]   0% Done                                    \rCopying gs://indaba-data/test/test_mut.pt...\n",
            "/ [0/9 files][    0.0 B/  3.3 GiB]   0% Done                                    \rCopying gs://indaba-data/README.txt...\n",
            "/ [0/9 files][    0.0 B/  3.3 GiB]   0% Done                                    \rCopying gs://indaba-data/train/train.csv...\n",
            "/ [0/9 files][    0.0 B/  3.3 GiB]   0% Done                                    \rCopying gs://indaba-data/train/train_mut.pt...\n",
            "/ [0/9 files][    0.0 B/  3.3 GiB]   0% Done                                    \rCopying gs://indaba-data/train/train_wt.pt...\n",
            "/ [0/9 files][    0.0 B/  3.3 GiB]   0% Done                                    \rCopying gs://indaba-data/test/test_wt.pt...\n",
            "/ [0/9 files][    0.0 B/  3.3 GiB]   0% Done                                    \r==> NOTE: You are downloading one or more large file(s), which would\n",
            "run significantly faster if you enabled sliced object downloads. This\n",
            "feature is enabled by default but requires that compiled crcmod be\n",
            "installed (see \"gsutil help crcmod\").\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Imports and moving to working directory\n",
        "import torch \n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "from sklearn.model_selection import train_test_split\n",
        "import torch.nn as nn\n",
        "\n",
        "# move to data folder\n",
        "%cd /content/indaba-data"
      ],
      "metadata": {
        "id": "Jvd8ERpgTvji",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "389302ec-7e06-4401-c103-daee7775c1c1"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/indaba-data\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load Embedding tensors & Traing csv\n",
        "# Embeddings were calculated using the ESM 650M pretrained model \n",
        "# Tensor shape of embedded data:  [data_len,1280] \n",
        "# There are no sequences in the Embedding tensors as we've performed an average of it (torch.mean(embed, dim=1))\n",
        "# More details in https://huggingface.co/facebook/esm2_t33_650M_UR50D\n",
        "\n",
        "wt_emb = torch.load(\"train/train_wt.pt\")\n",
        "mut_emb = torch.load(\"train/train_mut.pt\")\n",
        "df = pd.read_csv(\"train/train.csv\")"
      ],
      "metadata": {
        "id": "36ZgVoj5odV4"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# [Recommended] Split data into train and validation \n",
        "#TODO\n",
        "\n",
        "# Load embedding tensors\n",
        "train_mut_embeddings = torch.load('train/train_mut.pt')\n",
        "train_wt_embeddings = torch.load('train/train_wt.pt')\n",
        "\n",
        "# Load training CSV\n",
        "train_data = pd.read_csv('train/train.csv')\n",
        "\n",
        "# Splitting the data into train and validation sets\n",
        "train_data, val_data = train_test_split(train_data, test_size=0.2, random_state=42)\n",
        "\n",
        "# Printing the number of samples in each set\n",
        "print(\"Number of samples in the training set:\", len(train_data))\n",
        "print(\"Number of samples in the validation set:\", len(val_data))"
      ],
      "metadata": {
        "id": "zr0Njii0pRvN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e6f7afa4-5a6a-4067-cc9e-9c290d5b57b3"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of samples in the training set: 271822\n",
            "Number of samples in the validation set: 67956\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Building the dataset class\n",
        "class EmbeddingDataset(torch.utils.data.Dataset):\n",
        "  def __init__(self,mut_pt, wt_pt, data_df):\n",
        "    self.pt_mut = mut_pt\n",
        "    self.pt_wt = wt_pt\n",
        "    self.df = data_df\n",
        "  \n",
        "  def __len__(self):\n",
        "      return self.pt_mut.shape[0]\n",
        "\n",
        "  def __getitem__(self, index):\n",
        "    o1=self.pt_mut[index,:]\n",
        "    o2=self.pt_wt[index,:]\n",
        "    if \"ddg\" in self.df:\n",
        "      df_out=torch.Tensor([self.df[\"ddg\"][index]])\n",
        "    else:\n",
        "      df_out=torch.Tensor([self.df[\"ID\"][index]])\n",
        "    return  self.pt_mut[index,:],self.pt_wt[index,:],df_out "
      ],
      "metadata": {
        "id": "BEEH-ZWJgdUv"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # creating training dataset and dataloader\n",
        "# train_dataset = EmbeddingDataset(wt_emb, mut_emb, df)\n",
        "# # preparing a dataloader for the training\n",
        "# train_dataloader = torch.utils.data.dataloader.DataLoader(\n",
        "#         train_dataset,\n",
        "#         batch_size=32,\n",
        "#         shuffle=False,\n",
        "#         num_workers=2,\n",
        "#     )\n",
        "# # [Recommended] Use Data validation loader too\n",
        "\n",
        "\n",
        "# creating training dataset and dataloader\n",
        "train_dataset = EmbeddingDataset(wt_emb, mut_emb, df)\n",
        "# preparing a dataloader for the training\n",
        "train_dataloader = torch.utils.data.dataloader.DataLoader(\n",
        "        train_dataset,\n",
        "        batch_size=32,\n",
        "        shuffle=False,\n",
        "        num_workers=2,\n",
        "    )\n",
        "# [Recommended] Use Data validation loader too\n",
        "train_dataset = EmbeddingDataset(train_mut_embeddings, train_wt_embeddings, train_data)\n",
        "val_dataset = EmbeddingDataset(train_mut_embeddings, train_wt_embeddings, val_data)\n",
        "\n",
        "batch_size = 32\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=batch_size)\n"
      ],
      "metadata": {
        "id": "RrRVCI8siRfF"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # Building a simple pytorch model\n",
        "# # A dummy model (2-param) that demonstrates the usage of the dataset\n",
        "\n",
        "# class StabilityModel(torch.nn.Module):\n",
        "#   def __init__(self):\n",
        "#     super(StabilityModel, self).__init__()\n",
        "#     self.lin = torch.nn.Linear(1,1)\n",
        "\n",
        "#   def forward(self, x, y):\n",
        "#     # run the forward pass\n",
        "#     # output should be the stability estimation [batch,estim]\n",
        "#     return self.lin(torch.mean(x-y,dim=1).reshape(-1,1)) \n",
        "\n",
        "# class ProteinModel(nn.Module):\n",
        "#     def __init__(self, input_dim):\n",
        "#         super(ProteinModel, self).__init__()\n",
        "#         self.fc1 = nn.Linear(input_dim * 2, 256)\n",
        "#         self.fc2 = nn.Linear(256, 128)\n",
        "#         self.fc3 = nn.Linear(128, 1)\n",
        "#         self.relu = nn.ReLU()\n",
        "        \n",
        "#     def forward(self, mut_emb, wt_emb):\n",
        "#         x = torch.cat((mut_emb, wt_emb), dim=1)\n",
        "#         x = self.fc1(x)\n",
        "#         x = self.relu(x)\n",
        "#         x = self.fc2(x)\n",
        "#         x = self.relu(x)\n",
        "#         output = self.fc3(x)\n",
        "#         return output.squeeze(1)\n",
        "\n",
        "class ProteinModel(nn.Module):\n",
        "    def __init__(self, input_dim):\n",
        "        super(ProteinModel, self).__init__()\n",
        "        self.fc1 = nn.Linear(input_dim * 2, 512)\n",
        "        self.fc2 = nn.Linear(512, 256)\n",
        "        self.fc3 = nn.Linear(256, 128)\n",
        "        self.fc4 = nn.Linear(128, 64)\n",
        "        self.fc5 = nn.Linear(64, 1)\n",
        "        self.relu = nn.ReLU()\n",
        "\n",
        "    def forward(self, mut_emb, wt_emb):\n",
        "        x = torch.cat((mut_emb, wt_emb), dim=1)\n",
        "        x = self.fc1(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.fc2(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.fc3(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.fc4(x)\n",
        "        x = self.relu(x)\n",
        "        output = self.fc5(x)\n",
        "        return output.squeeze(1)\n",
        "\n",
        "\n",
        "# Instantiate the model\n",
        "input_dim = 1280  # Update with the actual input dimension\n",
        "model = ProteinModel(input_dim)\n",
        "\n",
        "# Set the model to evaluation mode\n",
        "model.eval()\n"
      ],
      "metadata": {
        "id": "9NuUlQRK8gHF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1d0622be-1cfa-402e-c2e5-b8f12b2bfede"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ProteinModel(\n",
              "  (fc1): Linear(in_features=2560, out_features=512, bias=True)\n",
              "  (fc2): Linear(in_features=512, out_features=256, bias=True)\n",
              "  (fc3): Linear(in_features=256, out_features=128, bias=True)\n",
              "  (fc4): Linear(in_features=128, out_features=64, bias=True)\n",
              "  (fc5): Linear(in_features=64, out_features=1, bias=True)\n",
              "  (relu): ReLU()\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# # Example of training script\n",
        "# device = torch.device(\"cuda\")\n",
        "# model =  StabilityModel().to(device)\n",
        "# optimizer = torch.optim.Adadelta(model.parameters(), lr=0.0001)\n",
        "# criterion = torch.nn.MSELoss()\n",
        "# epoch_loss = 0\n",
        "# for i in range(1):\n",
        "#   epoch_loss = 0\n",
        "#   for batch_idx, (data_mut,data_wt , target) in tqdm(enumerate(train_dataloader)):\n",
        "#       # extract input from datallader\n",
        "#       x1 = data_wt.to(device)\n",
        "#       x2 = data_mut.to(device)\n",
        "#       y = target.to(device)\n",
        "#       # make prediction\n",
        "#       y_pred = model(x1,x2)\n",
        "#       # calculate loss and run optimizer\n",
        "#       loss = torch.sqrt(criterion(y, y_pred))\n",
        "#       loss.backward()\n",
        "#       optimizer.step()\n",
        "#       epoch_loss += loss\n",
        "#   print(\"epoch_\",i,\" = \", epoch_loss/len(train_dataloader))\n",
        "# #   # [Recommended] Save trained models to select best checkpoint for prediction (or add prediction in the epochs loop)\n",
        "\n",
        "# # Set device\n",
        "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "\n",
        "\n",
        "# # Define hyperparameters\n",
        "# learning_rate = 0.001\n",
        "# batch_size = 32\n",
        "# num_epochs = 10\n",
        "\n",
        "# # Create instances of your model and move them to the device\n",
        "# model = YourModel().to(device)\n",
        "\n",
        "# # Define the loss function\n",
        "# loss_fn = nn.MSELoss()\n",
        "\n",
        "# # Define the optimizer\n",
        "# learning_rate = 0.001\n",
        "\n",
        "# model = ProteinModel(input_dim)  # Instantiate the model with the appropriate input dimension\n",
        "# optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "\n",
        "# # Create DataLoader instances for your training and validation datasets\n",
        "# train_dataset = EmbeddingDataset(train_mut_embeddings, train_wt_embeddings, train_data)\n",
        "# train_loader = tqdm(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "# val_dataset = EmbeddingDataset(train_mut_embeddings, train_wt_embeddings, val_data)\n",
        "# val_loader = DataLoader(val_dataset, batch_size=batch_size)\n",
        "\n",
        "# # Training loop\n",
        "# for epoch in range(num_epochs):\n",
        "#     # Set model to training mode\n",
        "#     model.train()\n",
        "\n",
        "#     for batch in train_loader:\n",
        "#         # Move batch to device\n",
        "#         batch = [item.to(device) for item in batch]\n",
        "\n",
        "#         # Forward pass\n",
        "#         output = model(batch[0], batch[1])\n",
        "\n",
        "#         # Compute loss\n",
        "#         loss = loss_fn(output, batch[2])\n",
        "\n",
        "#         # Backward pass and optimization\n",
        "#         optimizer.zero_grad()\n",
        "#         loss.backward()\n",
        "#         optimizer.step()\n",
        "\n",
        "#     # Set model to evaluation mode\n",
        "#     model.eval()\n",
        "\n",
        "#     # Validation\n",
        "#     with torch.no_grad():\n",
        "#         val_loss = 0.0\n",
        "#         num_val_samples = 0\n",
        "\n",
        "#         for batch in val_loader:\n",
        "#             # Move batch to device\n",
        "#             batch = [item.to(device) for item in batch]\n",
        "\n",
        "#             # Forward pass\n",
        "#             output = model(batch[0], batch[1])\n",
        "\n",
        "#             # Compute loss\n",
        "#             val_loss += loss_fn(output, batch[2]).item() * batch[0].size(0)\n",
        "#             num_val_samples += batch[0].size(0)\n",
        "\n",
        "#         val_loss /= num_val_samples\n",
        "\n",
        "#     # Print training and validation loss for each epoch\n",
        "#     print(f\"Epoch {epoch+1}/{num_epochs}, Training Loss: {loss.item():.4f}, Validation Loss: {val_loss:.4f}\")\n",
        "\n",
        "# # Save the trained model\n",
        "# torch.save(model.state_dict(), \"trained_model.pt\")\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "model = ProteinModel(input_dim=1280).to(device)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "criterion = torch.nn.MSELoss()\n",
        "\n",
        "num_epochs = 10\n",
        "\n",
        "train_losses = []\n",
        "val_losses = []\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    train_loss = 0.0\n",
        "    \n",
        "    for batch_idx, (data_mut, data_wt, target) in tqdm(enumerate(train_dataloader), total=len(train_dataloader), desc=f\"Epoch {epoch+1}\"):\n",
        "        data_mut = data_mut.to(device)\n",
        "        data_wt = data_wt.to(device)\n",
        "        target = target.to(device)\n",
        "        \n",
        "        optimizer.zero_grad()\n",
        "        \n",
        "        # Forward pass\n",
        "        output = model(data_mut, data_wt)\n",
        "        \n",
        "        # Compute loss\n",
        "        loss = torch.sqrt(criterion(output, target))\n",
        "        \n",
        "        # Backward pass\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        \n",
        "        train_loss += loss.item()\n",
        "    \n",
        "    avg_train_loss = train_loss / len(train_dataloader)\n",
        "    train_losses.append(avg_train_loss)\n",
        "    \n",
        "    model.eval()\n",
        "    val_loss = 0.0\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        for batch_idx, (data_mut, data_wt, target) in tqdm(enumerate(test_dataloader), total=len(test_dataloader), desc=\"Validation\"):\n",
        "            data_mut = data_mut.to(device)\n",
        "            data_wt = data_wt.to(device)\n",
        "            target = target.to(device)\n",
        "            \n",
        "            output = model(data_mut, data_wt)\n",
        "            loss = torch.sqrt(criterion(output, target))\n",
        "            val_loss += loss.item()\n",
        "        \n",
        "    avg_val_loss = val_loss / len(test_dataloader)\n",
        "    val_losses.append(avg_val_loss)\n",
        "    \n",
        "    print(f\"Epoch {epoch+1}: Train Loss = {avg_train_loss}, Validation Loss = {avg_val_loss}\")\n",
        "\n",
        "# Save the trained model\n",
        "torch.save(model.state_dict(), \"trained_model.pth\")\n",
        "\n",
        "\n",
        "\n",
        "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "# model = ProteinModel(input_dim).to(device)\n",
        "# optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "# criterion = torch.nn.MSELoss()\n",
        "\n",
        "# num_epochs = 10\n",
        "\n",
        "# for epoch in range(num_epochs):\n",
        "#     epoch_loss = 0\n",
        "#     model.train()\n",
        "    \n",
        "#     for batch_idx, (data_mut, data_wt, target) in tqdm(enumerate(train_dataloader), total=len(train_dataloader), desc=f\"Epoch {epoch+1}\"):\n",
        "#         data_mut = data_mut.to(device)\n",
        "#         data_wt = data_wt.to(device)\n",
        "#         target = target.to(device)\n",
        "        \n",
        "#         optimizer.zero_grad()\n",
        "        \n",
        "#         # Forward pass\n",
        "#         output = model(data_mut, data_wt)\n",
        "        \n",
        "#         # Compute loss\n",
        "#         loss = criterion(output, target)\n",
        "#         epoch_loss += loss.item()\n",
        "        \n",
        "#         # Backward pass and optimization\n",
        "#         loss.backward()\n",
        "#         optimizer.step()\n",
        "    \n",
        "#     # Calculate average loss for the epoch\n",
        "#     epoch_loss /= len(train_dataloader)\n",
        "    \n",
        "#     # Print epoch loss\n",
        "#     print(f\"Epoch {epoch+1}: Loss = {epoch_loss}\")\n",
        "    \n",
        "#     # [Recommended] Save trained models to select the best checkpoint for prediction\n",
        "    \n",
        "#     # Validation (optional)\n",
        "#     model.eval()\n",
        "#     with torch.no_grad():\n",
        "#         val_loss = 0\n",
        "#         for val_data_mut, val_data_wt, val_target in test_dataloader:\n",
        "#             val_data_mut = val_data_mut.to(device)\n",
        "#             val_data_wt = val_data_wt.to(device)\n",
        "#             val_target = val_target.to(device)\n",
        "            \n",
        "#             val_output = model(val_data_mut, val_data_wt)\n",
        "#             val_loss += criterion(val_output, val_target).item()\n",
        "        \n",
        "#         val_loss /= len(test_dataloader)\n",
        "#         print(f\"Validation Loss: {val_loss}\")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "ndGfhMrjlmUE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "513eccdd-4b79-4b9e-cbb3-e0d36d2bdc54"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1: 100%|██████████| 10619/10619 [01:05<00:00, 161.09it/s]\n",
            "Validation: 100%|██████████| 60/60 [00:00<00:00, 143.48it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1: Train Loss = 0.9540475396486267, Validation Loss = 1274.801348368327\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "Epoch 2: 100%|██████████| 10619/10619 [01:07<00:00, 157.25it/s]\n",
            "Validation: 100%|██████████| 60/60 [00:00<00:00, 138.59it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 2: Train Loss = 0.9550004894828963, Validation Loss = 1274.7964968681335\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "Epoch 3: 100%|██████████| 10619/10619 [01:07<00:00, 157.90it/s]\n",
            "Validation: 100%|██████████| 60/60 [00:00<00:00, 142.87it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 3: Train Loss = 0.9557956455833034, Validation Loss = 1274.8113611857095\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "Epoch 4: 100%|██████████| 10619/10619 [01:06<00:00, 158.86it/s]\n",
            "Validation: 100%|██████████| 60/60 [00:00<00:00, 92.12it/s] "
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 4: Train Loss = 0.9563198345861975, Validation Loss = 1274.8239435831706\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "Epoch 5: 100%|██████████| 10619/10619 [01:05<00:00, 162.41it/s]\n",
            "Validation: 100%|██████████| 60/60 [00:00<00:00, 143.65it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 5: Train Loss = 0.9568719492436062, Validation Loss = 1274.8298149426778\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "Epoch 6: 100%|██████████| 10619/10619 [01:05<00:00, 162.87it/s]\n",
            "Validation: 100%|██████████| 60/60 [00:00<00:00, 147.03it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 6: Train Loss = 0.9570408030461767, Validation Loss = 1274.8303663889567\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "Epoch 7: 100%|██████████| 10619/10619 [01:05<00:00, 162.50it/s]\n",
            "Validation: 100%|██████████| 60/60 [00:00<00:00, 148.69it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 7: Train Loss = 0.9571265462936458, Validation Loss = 1274.8311617215475\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "Epoch 8: 100%|██████████| 10619/10619 [01:04<00:00, 163.60it/s]\n",
            "Validation: 100%|██████████| 60/60 [00:00<00:00, 147.86it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 8: Train Loss = 0.9571667461408623, Validation Loss = 1274.8299104690552\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "Epoch 9: 100%|██████████| 10619/10619 [01:04<00:00, 164.36it/s]\n",
            "Validation: 100%|██████████| 60/60 [00:00<00:00, 102.78it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 9: Train Loss = 0.9571366578197399, Validation Loss = 1274.8287664731345\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "Epoch 10: 100%|██████████| 10619/10619 [01:04<00:00, 165.39it/s]\n",
            "Validation: 100%|██████████| 60/60 [00:00<00:00, 153.90it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 10: Train Loss = 0.9571092692146373, Validation Loss = 1274.8277411460876\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Prediction & submission"
      ],
      "metadata": {
        "id": "9GDKutS_nKOT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# load embedding tensors & traing csv\n",
        "wt_test_emb = torch.load(\"test/test_wt.pt\")\n",
        "mut_test_emb = torch.load(\"test/test_mut.pt\")\n",
        "df_test = pd.read_csv(\"test/test.csv\")"
      ],
      "metadata": {
        "id": "ave_DDMp8fo9"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# creating testing dataset and loading the embedding\n",
        "test_dataset = EmbeddingDataset(wt_test_emb,mut_test_emb,df_test)\n",
        "# preparing a dataloader for the testing\n",
        "test_dataloader = torch.utils.data.dataloader.DataLoader(\n",
        "        test_dataset,\n",
        "        batch_size=32,\n",
        "        shuffle=False,\n",
        "        num_workers=2,\n",
        "    )"
      ],
      "metadata": {
        "id": "9Xmav2yhm_Di"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_result = pd.DataFrame()\n",
        "with torch.no_grad():\n",
        "  for batch_idx, (data_mut,data_wt , target) in tqdm(enumerate(test_dataloader)):\n",
        "    x1 = data_wt.to(device)\n",
        "    x2 = data_mut.to(device)\n",
        "    id = target.to(device)\n",
        "    # make prediction\n",
        "    y_pred = model(x1,x2)\n",
        "    df_result = pd.concat([df_result, pd.DataFrame({\"ID\":id.squeeze().cpu().numpy().astype(int) , \"ddg\" : y_pred.squeeze().cpu().numpy()})])"
      ],
      "metadata": {
        "id": "DiylsXvjqOul",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e2731fe8-d65b-40b8-cf26-88d06dbfbaa8"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "60it [00:00, 88.32it/s] \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_result.to_csv(\"submission.csv\",index=False)"
      ],
      "metadata": {
        "id": "FPm-a2USexgw"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "7PGmPkRmezal"
      },
      "execution_count": 12,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}